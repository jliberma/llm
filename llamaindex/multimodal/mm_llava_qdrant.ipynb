{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ea05db5-944c-4bad-80a6-54841ccc0d42",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/multi_modal/llava_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "# LlaVa Demo with LlamaIndex\n",
    "\n",
    "In this example, we illustrate how we use LlaVa for belowing tasks:\n",
    "* Retrieval Augmented Image Captioning\n",
    "* Pydantic Structured Output\n",
    "* Multi-Modal Retrieval-Augmented Generation (RAG) using Llava-13b\n",
    "\n",
    "Context for LLaVA: Large Language and Vision Assistant\n",
    "* [Website](https://llava-vl.github.io/)\n",
    "* [Paper](https://arxiv.org/abs/2304.08485)\n",
    "* [Github](https://github.com/haotian-liu/LLaVA)\n",
    "* LLaVA 13b is now supported in Replicate: [See here.](https://replicate.com/yorickvp/llava-13b)\n",
    "\n",
    "For LlamaIndex:\n",
    "LlaVa+Replicate enables us to run image understanding locally and combine the multi-modal knowledge with our RAG knowledge based system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6811a8c",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Image Captioning using Llava-13b\n",
    "### Using Replicate serving LLaVa model through LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72eaf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install unstructured replicate\n",
    "%pip install llama_index ftfy regex tqdm\n",
    "%pip install git+https://github.com/openai/CLIP.git\n",
    "%pip install torch torchvision\n",
    "%pip install matplotlib scikit-image\n",
    "%pip install -U qdrant_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c70c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "REPLICATE_API_TOKEN = \"\"  # Your Relicate API token here\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3b4020-7e57-4392-ba29-52a14debd822",
   "metadata": {},
   "source": [
    "## Perform Data Extraction from Tesla 10K file\n",
    "\n",
    "\n",
    "In these sections we use Unstructured to parse out the table and non-table elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2249e49b-fea3-424a-9d3a-955c968899a6",
   "metadata": {},
   "source": [
    "### Extract Elements\n",
    "\n",
    "We use Unstructured to extract table and non-table elements from the 10-K filing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f416d23-42ba-4ce7-8d10-28e309f591c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://www.dropbox.com/scl/fi/mlaymdy1ni1ovyeykhhuk/tesla_2021_10k.htm?rlkey=qf9k4zn0ejrbm716j0gg7r802&dl=1\" -O tesla_2021_10k.htm\n",
    "!wget \"https://docs.google.com/uc?export=download&id=1UU0xc3uLXs-WG0aDQSXjGacUkp142rLS\" -O texas.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8cbbfd-38df-4499-9bb2-36efdeeed42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file.flat_reader import FlatReader\n",
    "from pathlib import Path\n",
    "from llama_index.node_parser import (\n",
    "    UnstructuredElementNodeParser,\n",
    ")\n",
    "\n",
    "reader = FlatReader()\n",
    "docs_2021 = reader.load_data(Path(\"tesla_2021_10k.htm\"))\n",
    "node_parser = UnstructuredElementNodeParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b2c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "OPENAI_API_TOKEN = \"sk-\"\n",
    "openai.api_key = OPENAI_API_TOKEN  # add your openai api key here\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8fec9f-ff94-468b-b930-c8b33773a720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "if not os.path.exists(\"2021_nodes.pkl\"):\n",
    "    raw_nodes_2021 = node_parser.get_nodes_from_documents(docs_2021)\n",
    "    pickle.dump(raw_nodes_2021, open(\"2021_nodes.pkl\", \"wb\"))\n",
    "else:\n",
    "    raw_nodes_2021 = pickle.load(open(\"2021_nodes.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea5181d-35e5-4f7d-9401-9eef95330ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_nodes_2021, node_mappings_2021 = node_parser.get_base_nodes_and_mappings(\n",
    "    raw_nodes_2021\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57035fe2-0ce8-460f-8a6c-2f0bc37d71d3",
   "metadata": {},
   "source": [
    "## Setup Recursive Retriever\n",
    "\n",
    "Now that we've extracted tables and their summaries, we can setup a recursive retriever in LlamaIndex to query these tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2393a9e-c1f2-452c-9683-61435c848fec",
   "metadata": {},
   "source": [
    "### Construct Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb2a4ff-0185-47b7-b68b-d2ba32242f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers import RecursiveRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index import VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7571128-c61d-42f1-af33-0aade58ee56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct top-level vector index + query engine\n",
    "vector_index = VectorStoreIndex(base_nodes_2021)\n",
    "vector_retriever = vector_index.as_retriever(similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b94a3e-e7de-4815-9598-a39834d40b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers import RecursiveRetriever\n",
    "\n",
    "recursive_retriever = RecursiveRetriever(\n",
    "    \"vector\",\n",
    "    retriever_dict={\"vector\": vector_retriever},\n",
    "    node_dict=node_mappings_2021,\n",
    "    verbose=True,\n",
    ")\n",
    "recursive_query_engine = RetrieverQueryEngine.from_args(recursive_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883d2d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "imageUrl = \"./texas.jpg\"\n",
    "image = Image.open(imageUrl).convert(\"RGB\")\n",
    "\n",
    "plt.figure(figsize=(16, 5))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29bf153",
   "metadata": {},
   "source": [
    "### Running LLaVa model using Replicate through LlamaIndex for image understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a21d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.multi_modal_llms import ReplicateMultiModal\n",
    "from llama_index.schema import ImageDocument\n",
    "from llama_index.multi_modal_llms.replicate_multi_modal import (\n",
    "    REPLICATE_MULTI_MODAL_LLM_MODELS,\n",
    ")\n",
    "\n",
    "print(imageUrl)\n",
    "\n",
    "llava_multi_modal_llm = ReplicateMultiModal(\n",
    "    model=REPLICATE_MULTI_MODAL_LLM_MODELS[\"llava-13b\"],\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "prompt = \"which Tesla factory is shown in the image? Please answer just the name of the factory.\"\n",
    "\n",
    "llava_response = llava_multi_modal_llm.complete(\n",
    "    prompt=prompt,\n",
    "    image_documents=[ImageDocument(image_path=imageUrl)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddae2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llava_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f15269-5903-458d-8a1c-55f68a3732cb",
   "metadata": {},
   "source": [
    "### Retrieve relevant information from LlamaIndex knowledge base based on LLaVa image understanding to augment `Image Captioning`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a998a0d7-be3a-4c39-ac94-43ecd070455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_response = recursive_query_engine.query(llava_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b439da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rag_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1346e70",
   "metadata": {},
   "source": [
    "## Multi-Modal Pydantic Program with LLaVa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fec1d1b",
   "metadata": {},
   "source": [
    "### Initialize the Instagram Ads Pydantic Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba06d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image_path = Path(\"instagram_images\")\n",
    "if not input_image_path.exists():\n",
    "    Path.mkdir(input_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffded4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://docs.google.com/uc?export=download&id=12ZpBBFkYu-jzz1iz356U5kMikn4uN9ww\" -O ./instagram_images/jordan.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca545bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class InsAds(BaseModel):\n",
    "    \"\"\"Data model for a Ins Ads.\"\"\"\n",
    "\n",
    "    account: str\n",
    "    brand: str\n",
    "    product: str\n",
    "    category: str\n",
    "    discount: str\n",
    "    price: str\n",
    "    comments: str\n",
    "    review: str\n",
    "    description: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abb9c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ins_imageUrl = \"./instagram_images/jordan.png\"\n",
    "image = Image.open(ins_imageUrl).convert(\"RGB\")\n",
    "\n",
    "plt.figure(figsize=(16, 5))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091908ef",
   "metadata": {},
   "source": [
    "### Using Multi-Modal Pydantic Program to generate structured output using Llava-13b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01e253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.multi_modal_llms import ReplicateMultiModal\n",
    "from llama_index.program import MultiModalLLMCompletionProgram\n",
    "from llama_index.output_parsers import PydanticOutputParser\n",
    "from llama_index.multi_modal_llms.replicate_multi_modal import (\n",
    "    REPLICATE_MULTI_MODAL_LLM_MODELS,\n",
    ")\n",
    "\n",
    "prompt_template_str = \"\"\"\\\n",
    "    can you summarize what is in the image\\\n",
    "    and return the answer with json format \\\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def pydantic_llava(\n",
    "    model_name, output_class, image_documents, prompt_template_str\n",
    "):\n",
    "    mm_llm = ReplicateMultiModal(\n",
    "        model=REPLICATE_MULTI_MODAL_LLM_MODELS[\"llava-13b\"],\n",
    "        max_new_tokens=1000,\n",
    "    )\n",
    "\n",
    "    llm_program = MultiModalLLMCompletionProgram.from_defaults(\n",
    "        output_parser=PydanticOutputParser(output_class),\n",
    "        image_documents=image_documents,\n",
    "        prompt_template_str=prompt_template_str,\n",
    "        multi_modal_llm=mm_llm,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    response = llm_program()\n",
    "    print(f\"Model: {model_name}\")\n",
    "    for res in response:\n",
    "        print(res)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4676d110",
   "metadata": {},
   "source": [
    "### Output Structured Pydantic Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4a8da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "ins_image_documents = SimpleDirectoryReader(\"./instagram_images\").load_data()\n",
    "\n",
    "pydantic_response = pydantic_llava(\n",
    "    \"llava-13b\", InsAds, ins_image_documents, prompt_template_str\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a94fb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pydantic_response.brand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c30e720",
   "metadata": {},
   "source": [
    "## Advanced Multi-Modal Retrieval using GPT4V and Multi-Modal Index/Retriever/Query Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6410d6",
   "metadata": {},
   "source": [
    "### Downloading text, images data from raw files [Wikipedia] for Multi Modal Index/Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9e1517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "wiki_titles = [\n",
    "    \"batman\",\n",
    "    \"Vincent van Gogh\",\n",
    "    \"San Francisco\",\n",
    "    \"iPhone\",\n",
    "    \"Tesla Model S\",\n",
    "    \"BTS\",\n",
    "    \"Air Jordan\",\n",
    "]\n",
    "\n",
    "\n",
    "data_path = Path(\"data_wiki\")\n",
    "\n",
    "\n",
    "for title in wiki_titles:\n",
    "    response = requests.get(\n",
    "        \"https://en.wikipedia.org/w/api.php\",\n",
    "        params={\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": title,\n",
    "            \"prop\": \"extracts\",\n",
    "            \"explaintext\": True,\n",
    "        },\n",
    "    ).json()\n",
    "    page = next(iter(response[\"query\"][\"pages\"].values()))\n",
    "    wiki_text = page[\"extract\"]\n",
    "\n",
    "    if not data_path.exists():\n",
    "        Path.mkdir(data_path)\n",
    "\n",
    "    with open(data_path / f\"{title}.txt\", \"w\") as fp:\n",
    "        fp.write(wiki_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c1ebf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import urllib.request\n",
    "\n",
    "image_path = Path(\"data_wiki\")\n",
    "image_uuid = 0\n",
    "# image_metadata_dict stores images metadata including image uuid, filename and path\n",
    "image_metadata_dict = {}\n",
    "MAX_IMAGES_PER_WIKI = 30\n",
    "\n",
    "wiki_titles = [\n",
    "    \"Air Jordan\",\n",
    "    \"San Francisco\",\n",
    "    \"Batman\",\n",
    "    \"Vincent van Gogh\",\n",
    "    \"iPhone\",\n",
    "    \"Tesla Model S\",\n",
    "    \"BTS band\",\n",
    "]\n",
    "\n",
    "# create folder for images only\n",
    "if not image_path.exists():\n",
    "    Path.mkdir(image_path)\n",
    "\n",
    "# Download images for wiki pages\n",
    "# Assing UUID for each image\n",
    "for title in wiki_titles:\n",
    "    images_per_wiki = 0\n",
    "    print(title)\n",
    "    try:\n",
    "        page_py = wikipedia.page(title)\n",
    "        list_img_urls = page_py.images\n",
    "        for url in list_img_urls:\n",
    "            if url.endswith(\".jpg\") or url.endswith(\".png\"):\n",
    "                image_uuid += 1\n",
    "                image_file_name = title + \"_\" + url.split(\"/\")[-1]\n",
    "\n",
    "                # img_path could be s3 path pointing to the raw image file in the future\n",
    "                image_metadata_dict[image_uuid] = {\n",
    "                    \"filename\": image_file_name,\n",
    "                    \"img_path\": \"./\" + str(image_path / f\"{image_uuid}.jpg\"),\n",
    "                }\n",
    "                urllib.request.urlretrieve(\n",
    "                    url, image_path / f\"{image_uuid}.jpg\"\n",
    "                )\n",
    "                images_per_wiki += 1\n",
    "                # Limit the number of images downloaded per wiki page to 15\n",
    "                if images_per_wiki > MAX_IMAGES_PER_WIKI:\n",
    "                    break\n",
    "    except:\n",
    "        print(str(Exception(\"No images found for Wikipedia page: \")) + title)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22fb47f",
   "metadata": {},
   "source": [
    "### Build Multi-modal index and Vector Store to index both text and images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a973fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "from llama_index import (\n",
    "    ServiceContext,\n",
    "    SimpleDirectoryReader,\n",
    ")\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index import VectorStoreIndex, StorageContext\n",
    "from llama_index.indices.multi_modal.base import MultiModalVectorStoreIndex\n",
    "\n",
    "# Create a local Qdrant vector store\n",
    "#client = qdrant_client.QdrantClient(path=\"qdrant_mm_db\")\n",
    "client = qdrant_client.QdrantClient(url=\"http://localhost:6333\")\n",
    "                                    \n",
    "text_store = QdrantVectorStore(\n",
    "    client=client, collection_name=\"text_collection\"\n",
    ")\n",
    "image_store = QdrantVectorStore(\n",
    "    client=client, collection_name=\"image_collection\"\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=text_store, image_store=image_store\n",
    ")\n",
    "\n",
    "# Create the MultiModal index\n",
    "documents = SimpleDirectoryReader(\"./data_wiki/\").load_data()\n",
    "index = MultiModalVectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36302a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "def plot_images(image_metadata_dict):\n",
    "    original_images_urls = []\n",
    "    images_shown = 0\n",
    "    for image_id in image_metadata_dict:\n",
    "        img_path = image_metadata_dict[image_id][\"img_path\"]\n",
    "        if os.path.isfile(img_path):\n",
    "            filename = image_metadata_dict[image_id][\"filename\"]\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "            plt.subplot(8, 8, len(original_images_urls) + 1)\n",
    "            plt.imshow(image)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "            original_images_urls.append(filename)\n",
    "            images_shown += 1\n",
    "            if images_shown >= 64:\n",
    "                break\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "plot_images(image_metadata_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea72cd4",
   "metadata": {},
   "source": [
    "### Multi-Modal RAG Retrieval and Querying using LlaVa pydantic structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbaeae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate  retrieval results\n",
    "retriever = index.as_retriever(similarity_top_k=3, image_similarity_top_k=5)\n",
    "retrieval_results = retriever.retrieve(pydantic_response.brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304e7f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.response.notebook_utils import (\n",
    "    display_source_node,\n",
    "    display_image_uris,\n",
    ")\n",
    "from llama_index.schema import ImageNode\n",
    "\n",
    "retrieved_image = []\n",
    "for res_node in retrieval_results:\n",
    "    if isinstance(res_node.node, ImageNode):\n",
    "        retrieved_image.append(res_node.node.metadata[\"file_path\"])\n",
    "    else:\n",
    "        display_source_node(res_node, source_length=200)\n",
    "\n",
    "\n",
    "display_image_uris(retrieved_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3c81fb",
   "metadata": {},
   "source": [
    "### Synthesis the RAG results using retrieved texts and images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bcb3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.prompts import PromptTemplate\n",
    "from llama_index.query_engine import SimpleMultiModalQueryEngine\n",
    "\n",
    "qa_tmpl_str = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the query.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_tmpl = PromptTemplate(qa_tmpl_str)\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    multi_modal_llm=llava_multi_modal_llm,\n",
    "    text_qa_template=qa_tmpl,\n",
    "    similarity_top_k=2,\n",
    "    image_similarity_top_k=1,\n",
    ")\n",
    "\n",
    "query_str = \"Tell me more about the \" + pydantic_response.brand + \" brand.\"\n",
    "response = query_engine.query(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d202db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

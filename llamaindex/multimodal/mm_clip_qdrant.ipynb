{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bbc9a0e",
   "metadata": {
    "id": "3bbc9a0e"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/multi_modal/multi_modal_retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Multi-Modal Retrieval using GPT text embedding and CLIP image embedding for Wikipedia Articles\n",
    "\n",
    "In this notebook, we show how to build a Multi-Modal retrieval system using LlamaIndex.\n",
    "\n",
    "Wikipedia Text embedding index: Generate GPT text embeddings from OpenAI for texts\n",
    "\n",
    "Wikipedia Images embedding index: [CLIP](https://github.com/openai/CLIP) embeddings from OpenAI for images\n",
    "\n",
    "\n",
    "Query encoder:\n",
    "* Encoder query text for text index using GPT embedding\n",
    "* Encoder query text for image index using CLIP embedding\n",
    "\n",
    "Framework: [LlamaIndex](https://github.com/run-llama/llama_index)\n",
    "\n",
    "Steps:\n",
    "1. Download texts and images raw files for Wikipedia articles\n",
    "2. Build text index for vector store using GPT embeddings\n",
    "3. Build image index for vector store using CLIP embeddings\n",
    "4. Retrieve relevant text and image simultaneously using different query encoding embeddings and vector stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc691ca8",
   "metadata": {
    "id": "fc691ca8"
   },
   "outputs": [],
   "source": [
    "%pip install llama_index ftfy regex tqdm\n",
    "%pip install git+https://github.com/openai/CLIP.git\n",
    "%pip install torch torchvision\n",
    "%pip install matplotlib scikit-image\n",
    "%pip install -U qdrant_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbb2fc8",
   "metadata": {
    "id": "5dbb2fc8"
   },
   "source": [
    "## Load and Download Multi-Modal datasets including texts and images from Wikipedia\n",
    "Parse wikipedia articles and save into local folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4294270",
   "metadata": {
    "id": "e4294270"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "wiki_titles = [\n",
    "    \"batman\",\n",
    "    \"Vincent van Gogh\",\n",
    "    \"San Francisco\",\n",
    "    \"iPhone\",\n",
    "    \"Tesla Model S\",\n",
    "    \"BTS\",\n",
    "]\n",
    "\n",
    "\n",
    "data_path = Path(\"data_wiki\")\n",
    "\n",
    "for title in wiki_titles:\n",
    "    response = requests.get(\n",
    "        \"https://en.wikipedia.org/w/api.php\",\n",
    "        params={\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": title,\n",
    "            \"prop\": \"extracts\",\n",
    "            \"explaintext\": True,\n",
    "        },\n",
    "    ).json()\n",
    "    page = next(iter(response[\"query\"][\"pages\"].values()))\n",
    "    wiki_text = page[\"extract\"]\n",
    "\n",
    "    if not data_path.exists():\n",
    "        Path.mkdir(data_path)\n",
    "\n",
    "    with open(data_path / f\"{title}.txt\", \"w\") as fp:\n",
    "        fp.write(wiki_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e62f6d",
   "metadata": {
    "id": "90e62f6d"
   },
   "source": [
    "## Parse Wikipedia Images and texts. Load into local folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8305a45a",
   "metadata": {
    "id": "8305a45a"
   },
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import urllib.request\n",
    "\n",
    "image_path = Path(\"data_wiki\")\n",
    "image_uuid = 0\n",
    "# image_metadata_dict stores images metadata including image uuid, filename and path\n",
    "image_metadata_dict = {}\n",
    "MAX_IMAGES_PER_WIKI = 10\n",
    "\n",
    "wiki_titles = [\n",
    "    \"San Francisco\",\n",
    "    \"Batman\",\n",
    "    \"Vincent van Gogh\",\n",
    "    \"iPhone\",\n",
    "    \"Tesla Model S\",\n",
    "    \"BTS band\",\n",
    "]\n",
    "\n",
    "# create folder for images only\n",
    "if not image_path.exists():\n",
    "    Path.mkdir(image_path)\n",
    "\n",
    "\n",
    "# Download images for wiki pages\n",
    "# Assing UUID for each image\n",
    "for title in wiki_titles:\n",
    "    images_per_wiki = 0\n",
    "    #print(title)\n",
    "    try:\n",
    "        page_py = wikipedia.page(title)\n",
    "        list_img_urls = page_py.images\n",
    "        #print(list_img_urls)\n",
    "        for url in list_img_urls:\n",
    "            if url.endswith(\".jpg\") or url.endswith(\".png\"):\n",
    "                image_uuid += 1\n",
    "                image_file_name = title + \"_\" + url.split(\"/\")[-1]\n",
    "                #print(image_file_name)\n",
    "                # img_path could be s3 path pointing to the raw image file in the future\n",
    "                image_metadata_dict[image_uuid] = {\n",
    "                    \"filename\": image_file_name,\n",
    "                    \"img_path\": \"./\" + str(image_path / f\"{image_uuid}.jpg\"),\n",
    "                }\n",
    "                urllib.request.urlretrieve(\n",
    "                    url, image_path / f\"{image_uuid}.jpg\"\n",
    "                )\n",
    "                #print(image_metadata_dict[image_uuid])\n",
    "                images_per_wiki += 1\n",
    "                # Limit the number of images downloaded per wiki page to 15\n",
    "                if images_per_wiki >= MAX_IMAGES_PER_WIKI:\n",
    "                    break\n",
    "    except:\n",
    "        print(str(Exception(\"No images found for Wikipedia page: \")) + title)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a9fcec8-cdc3-4887-9276-5f9a9febf820",
   "metadata": {
    "id": "26b31697"
   },
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f958e7a-b094-4769-92e7-db6b0c28e414",
   "metadata": {
    "id": "6976c3a7",
    "outputId": "a9f70611-36df-457d-8e9d-8045a64717c3"
   },
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "\n",
    "#from qdrant_client.http import models\n",
    "#from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "from llama_index import (\n",
    "    ServiceContext,\n",
    "    SimpleDirectoryReader,\n",
    ")\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index import VectorStoreIndex, StorageContext\n",
    "from llama_index.indices.multi_modal.base import MultiModalVectorStoreIndex\n",
    "\n",
    "# Create a local Qdrant vector store\n",
    "#client = qdrant_client.QdrantClient(path=\"qdrant_db\")\n",
    "client = qdrant_client.QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "text_store = QdrantVectorStore(\n",
    "    client=client, collection_name=\"text_collection\"\n",
    ")\n",
    "image_store = QdrantVectorStore(\n",
    "    client=client, collection_name=\"image_collection\"\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=text_store, image_store=image_store\n",
    ")\n",
    "\n",
    "# Create the MultiModal index\n",
    "documents = SimpleDirectoryReader(\"./data_wiki/\").load_data()\n",
    "index = MultiModalVectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f1eadb-7ee9-40a6-8566-3d4e74dafaa9",
   "metadata": {
    "id": "6976c3a7",
    "outputId": "a9f70611-36df-457d-8e9d-8045a64717c3"
   },
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "\n",
    "from llama_index import (\n",
    "    ServiceContext,\n",
    "    SimpleDirectoryReader,\n",
    ")\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index import VectorStoreIndex, StorageContext\n",
    "from llama_index.indices.multi_modal.base import MultiModalVectorStoreIndex\n",
    "\n",
    "# Create a HTTP Qdrant vector store\n",
    "client = qdrant_client.QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "text_store = QdrantVectorStore(\n",
    "    client=client, collection_name=\"text_collection\"\n",
    ")\n",
    "image_store = QdrantVectorStore(\n",
    "    client=client, collection_name=\"image_collection\"\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=text_store, image_store=image_store\n",
    ")\n",
    "\n",
    "# Create the MultiModal index\n",
    "documents = SimpleDirectoryReader(\"./data_wiki/\").load_data()\n",
    "index = MultiModalVectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    ")\n",
    "\n",
    "# View the collections\n",
    "collection_info = client.get_collection(collection_name=\"text_collection\")\n",
    "print(\"text_collection (vectors,indexed): (\", collection_info.vectors_count,\",\",collection_info.indexed_vectors_count,\")\")\n",
    "collection_info = client.get_collection(collection_name=\"image_collection\")\n",
    "print(\"image_collection (vectors,indexed): (\", collection_info.vectors_count,\",\",collection_info.indexed_vectors_count,\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd665d2",
   "metadata": {
    "id": "6cd665d2"
   },
   "source": [
    "### Plot downloaded Images from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca1e93c",
   "metadata": {
    "id": "eca1e93c",
    "outputId": "f21183d2-a8d4-490b-d22d-61e9d8b41585"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "def plot_images(image_metadata_dict):\n",
    "    original_images_urls = []\n",
    "    images_shown = 0\n",
    "    for image_id in image_metadata_dict:\n",
    "        img_path = image_metadata_dict[image_id][\"img_path\"]\n",
    "        if os.path.isfile(img_path):\n",
    "            filename = image_metadata_dict[image_id][\"filename\"]\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "            plt.subplot(8, 8, len(original_images_urls) + 1)\n",
    "            plt.imshow(image)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "            original_images_urls.append(filename)\n",
    "            images_shown += 1\n",
    "            if images_shown >= 64:\n",
    "                break\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "plot_images(image_metadata_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c74e51",
   "metadata": {
    "id": "56c74e51"
   },
   "source": [
    "### Build a separate CLIP image embedding index under a differnt collection `wikipedia_img`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acc0b06",
   "metadata": {
    "id": "6acc0b06"
   },
   "outputs": [],
   "source": [
    "def plot_images(image_paths):\n",
    "    images_shown = 0\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    for img_path in image_paths:\n",
    "        if os.path.isfile(img_path):\n",
    "            image = Image.open(img_path)\n",
    "\n",
    "            plt.subplot(2, 3, images_shown + 1)\n",
    "            plt.imshow(image)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "            images_shown += 1\n",
    "            if images_shown >= 9:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ed2a45-a5ab-4900-9d91-05ad7a10ad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "import numpy as np\n",
    "\n",
    "# load the CLIP model with the name ViT-B/32\n",
    "model, preprocess = clip.load(\"ViT-B/32\")\n",
    "\n",
    "# the resolution of the input images expected by the model\n",
    "input_resolution = model.visual.input_resolution\n",
    "\n",
    "# the maximum length of the input text\n",
    "context_length = model.context_length\n",
    "\n",
    "# the size of the vocabulary used by the model\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "# print the information about the model to the console\n",
    "print(\n",
    "   \"Model parameters:\",\n",
    "   f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\",\n",
    ")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4383a0b0-4bfb-4e88-ab3f-16fe15cecdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "\n",
    "# img_emb_dict stores image embeddings for each image\n",
    "img_emb_dict = {}\n",
    "with torch.no_grad():\n",
    "\n",
    "\n",
    "   # extracts image embeddings for each image from the metadata dictionary\n",
    "   for image_filename in image_metadata_dict:\n",
    "       img_file_path = image_metadata_dict[image_filename][\"img_path\"]\n",
    "       if os.path.isfile(img_file_path):\n",
    "           image = (\n",
    "               # preprocess the image using the CLIP model's preprocess function\n",
    "               # unsqueeze the image tensor to add a batch dimension\n",
    "               # move the image tensor to the device specified in line 1\n",
    "               preprocess(Image.open(img_file_path)).unsqueeze(0).to(device)\n",
    "           )\n",
    "\n",
    "\n",
    "           # extract image features using the CLIP model's encode_image function\n",
    "           image_features = model.encode_image(image)\n",
    "\n",
    "\n",
    "           # store the image features in the image embedding dictionary\n",
    "           img_emb_dict[image_filename] = image_features\n",
    "\n",
    "print(img_emb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9f229a-ca61-4063-ab5a-93655b4b63b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.schema import ImageDocument\n",
    "\n",
    "\n",
    "# create a ImageDocument list object for each image in the dataset \n",
    "img_documents = []\n",
    "for image_filename in image_metadata_dict:\n",
    "   # the img_emb_dict dictionary contains the image embeddings\n",
    "   if image_filename in img_emb_dict:\n",
    "       filename = image_metadata_dict[image_filename][\"filename\"]\n",
    "       filepath = image_metadata_dict[image_filename][\"img_path\"]\n",
    "       print(filepath)\n",
    "\n",
    "\n",
    "       # create an ImageDocument for each image\n",
    "       newImgDoc = ImageDocument(\n",
    "           text=filename, metadata={\"filepath\": filepath}\n",
    "       )\n",
    "\n",
    "\n",
    "       # set image embedding on the ImageDocument\n",
    "       newImgDoc.embedding = img_emb_dict[image_filename].tolist()[0]\n",
    "       img_documents.append(newImgDoc)\n",
    "\n",
    "\n",
    "# create QdrantVectorStore, with collection name \"CLIP_image collection\"\n",
    "wikipedia_store = QdrantVectorStore(\n",
    "   client=client, collection_name=\"CLIP image collection\"\n",
    ")\n",
    "\n",
    "\n",
    "# define storage context\n",
    "storage_context = StorageContext.from_defaults(vector_store=wikipedia_store)\n",
    "\n",
    "\n",
    "# define image index\n",
    "image_index = VectorStoreIndex.from_documents(\n",
    "   img_documents,\n",
    "   storage_context=storage_context\n",
    ")\n",
    "\n",
    "\n",
    "collection_info = client.get_collection(collection_name=\"CLIP image collection\")\n",
    "print(\"CLIP image collection (vectors,indexed): (\", collection_info.vectors_count,\",\",collection_info.indexed_vectors_count,\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0047b491-48ad-4e58-aa0e-0527992a54c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores import VectorStoreQuery\n",
    "\n",
    "\n",
    "# return the most similar image to a test query\n",
    "def retrieve_results_from_image_index(query):\n",
    "\n",
    "\n",
    "   # first tokenize the text query and convert it to a tensor\n",
    "   text = clip.tokenize(query).to(device)\n",
    "\n",
    "\n",
    "   # encode the text tensor using the CLIP model to produce a query embedding\n",
    "   query_embedding = model.encode_text(text).tolist()[0]\n",
    "\n",
    "\n",
    "   # create a VectorStoreQuery\n",
    "   image_vector_store_query = VectorStoreQuery(\n",
    "       query_embedding=query_embedding,\n",
    "       similarity_top_k=2, # only return 1 image\n",
    "       mode=\"default\",\n",
    "   )\n",
    "\n",
    "\n",
    "   # query the image vector store\n",
    "   image_retrieval_results = wikipedia_store.query(\n",
    "       image_vector_store_query\n",
    "   )\n",
    "   return image_retrieval_results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create a new 16 x 5 inch figure from retrieval results\n",
    "def plot_image_retrieve_results(image_retrieval_results):\n",
    "   plt.figure(figsize=(16, 5))\n",
    "\n",
    "\n",
    "   img_cnt = 0\n",
    "   # subplot the image and score for each retrieval result\n",
    "   for returned_image, score in zip(\n",
    "       image_retrieval_results.nodes, image_retrieval_results.similarities\n",
    "   ):\n",
    "       img_name = returned_image.text\n",
    "       img_path = returned_image.metadata[\"filepath\"]\n",
    "       image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "       plt.subplot(2, 3, img_cnt + 1)\n",
    "       plt.title(\"{:.4f}\".format(score))\n",
    "\n",
    "\n",
    "       plt.imshow(image)\n",
    "       plt.xticks([])\n",
    "       plt.yticks([])\n",
    "       img_cnt += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# define image_query function\n",
    "def image_query(query):\n",
    "   image_retrieval_results = retrieve_results_from_image_index(query)\n",
    "   plot_image_retrieve_results(image_retrieval_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea1e2fb",
   "metadata": {
    "id": "3ea1e2fb"
   },
   "source": [
    "## Get Multi-Modal retrieval results for some example queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1794daf9-3bcc-4b0e-a2b1-cf7cd87b54d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define text query engine\n",
    "text_query_engine = index.as_query_engine()\n",
    "\n",
    "query = \"Who is the main character of Batman?\"\n",
    "# generate Image retrieval results\n",
    "image_query(query)\n",
    "\n",
    "# generate Text retrieval results\n",
    "text_retrieval_results = text_query_engine.query(query)\n",
    "print(str(text_retrieval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ca6b07-e735-4584-9a0b-9454b0f9ef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define text query engine\n",
    "text_query_engine = index.as_query_engine()\n",
    "\n",
    "query = \"Which are Van Gogh's most famous paintings?\"\n",
    "# generate Image retrieval results\n",
    "image_query(query)\n",
    "\n",
    "# generate Text retrieval results\n",
    "text_retrieval_results = text_query_engine.query(query)\n",
    "print(str(text_retrieval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fdd3a0",
   "metadata": {
    "id": "61fdd3a0"
   },
   "outputs": [],
   "source": [
    "# define text query engine\n",
    "text_query_engine = index.as_query_engine()\n",
    "\n",
    "query = \"What are the most popular tourist attractions in San Francisco\"\n",
    "# generate Image retrieval results\n",
    "image_query(query)\n",
    "\n",
    "# generate Text retrieval results\n",
    "text_retrieval_results = text_query_engine.query(query)\n",
    "print(str(text_retrieval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b79d67c",
   "metadata": {
    "id": "8b79d67c",
    "outputId": "2de82fb6-ec41-4b35-8649-c04be2fe9dd7"
   },
   "outputs": [],
   "source": [
    "from llama_index.response.notebook_utils import display_source_node\n",
    "from llama_index.schema import ImageNode\n",
    "\n",
    "retrieved_image = []\n",
    "for res_node in retrieval_results:\n",
    "    if isinstance(res_node.node, ImageNode):\n",
    "        retrieved_image.append(res_node.node.metadata[\"file_path\"])\n",
    "    else:\n",
    "        display_source_node(res_node, source_length=200)\n",
    "\n",
    "plot_images(retrieved_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b2ca49-a266-4c88-9cab-3a5f2f4aacb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
